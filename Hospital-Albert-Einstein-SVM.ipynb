{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T12:56:25.452177Z",
     "start_time": "2021-06-08T12:56:15.539837Z"
    }
   },
   "outputs": [],
   "source": [
    "#%% Libraries\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import pandas as pds\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from pandas_profiling import ProfileReport\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score, plot_roc_curve\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import RFECV as RFECV_SKYLEARN\n",
    "pd.options.display.max_columns = 100\n",
    "#pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.display import Audio, display\n",
    "def allDone():\n",
    "    display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First test with all Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T12:56:30.814014Z",
     "start_time": "2021-06-08T12:56:30.719291Z"
    }
   },
   "outputs": [],
   "source": [
    "#Importing the necessary packages and libaries\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T12:56:32.648138Z",
     "start_time": "2021-06-08T12:56:32.625170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['med', 'med', 'med', 'med', 'med', ..., 'high', 'high', 'high', 'high', 'high']\n",
       "Length: 10000\n",
       "Categories (3, object): ['low' < 'med' < 'high']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Open File\n",
    "albert = pd.read_csv('C:/Users/user/Documents/1. GitHub/Albert_Einstein/data.csv')\n",
    "# Order categories\n",
    "categories = pd.Categorical(albert['target'], categories=['low', 'med', 'high'], ordered=True)\n",
    "categories\n",
    "# Label your target with numerical values\n",
    "labels, unique = pd.factorize(categories, sort=True)\n",
    "albert['target'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T12:56:36.950026Z",
     "start_time": "2021-06-08T12:56:36.944069Z"
    }
   },
   "outputs": [],
   "source": [
    "X = albert[['x1', 'x2', 'x3']]\n",
    "y = albert['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T13:26:56.994067Z",
     "start_time": "2021-06-08T12:56:52.639769Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "linear = svm.SVC(kernel='linear', C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
    "rbf = svm.SVC(kernel='rbf', gamma=1, C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
    "poly = svm.SVC(kernel='poly', degree=3, C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
    "sig = svm.SVC(kernel='sigmoid', C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T13:34:38.670205Z",
     "start_time": "2021-06-08T13:34:37.616415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Linear Kernel: 0.605\n",
      "Accuracy Polynomial Kernel: 0.605\n",
      "Accuracy Radial Basis Kernel: 0.5935\n",
      "Accuracy Sigmoid Kernel: 0.497\n"
     ]
    }
   ],
   "source": [
    "# retrieve the accuracy and print it for all 4 kernel functions\n",
    "accuracy_lin = linear.score(X_test, y_test)\n",
    "accuracy_poly = poly.score(X_test, y_test)\n",
    "accuracy_rbf = rbf.score(X_test, y_test)\n",
    "accuracy_sig = sig.score(X_test, y_test)\n",
    "print('Accuracy Linear Kernel:', accuracy_lin)\n",
    "print('Accuracy Polynomial Kernel:', accuracy_poly)\n",
    "print('Accuracy Radial Basis Kernel:', accuracy_rbf)\n",
    "print('Accuracy Sigmoid Kernel:', accuracy_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T13:36:22.112382Z",
     "start_time": "2021-06-08T13:36:21.060942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1210    0    0]\n",
      " [ 180    0    0]\n",
      " [ 610    0    0]]\n",
      "[[1210    0    0]\n",
      " [ 180    0    0]\n",
      " [ 610    0    0]]\n",
      "[[1164    3   43]\n",
      " [ 172    0    8]\n",
      " [ 587    0   23]]\n",
      "[[931 203  76]\n",
      " [144  27   9]\n",
      " [477  97  36]]\n"
     ]
    }
   ],
   "source": [
    "# Set predict variables\n",
    "linear_pred = linear.predict(X_test)\n",
    "poly_pred = poly.predict(X_test)\n",
    "rbf_pred = rbf.predict(X_test)\n",
    "sig_pred = sig.predict(X_test)\n",
    "# creating a confusion matrix\n",
    "cm_lin = confusion_matrix(y_test, linear_pred)\n",
    "cm_poly = confusion_matrix(y_test, poly_pred)\n",
    "cm_rbf = confusion_matrix(y_test, rbf_pred)\n",
    "cm_sig = confusion_matrix(y_test, sig_pred)\n",
    "print(cm_lin)\n",
    "print(cm_poly)\n",
    "print(cm_rbf)\n",
    "print(cm_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperarameters: tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-07T19:46:27.947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid=[{'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001],\n",
       "                          'kernel': ['rbf']},\n",
       "                         {'C': [1, 10, 100, 1000], 'kernel': ['linear']}],\n",
       "             scoring='precision_macro')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.200 (+/-0.000) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.200 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.200 (+/-0.000) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.200 (+/-0.000) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.200 (+/-0.000) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.200 (+/-0.000) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.200 (+/-0.000) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.200 (+/-0.000) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.200 (+/-0.000) for {'C': 1, 'kernel': 'linear'}\n",
      "0.200 (+/-0.000) for {'C': 10, 'kernel': 'linear'}\n",
      "0.200 (+/-0.000) for {'C': 100, 'kernel': 'linear'}\n",
      "0.200 (+/-0.000) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75      1801\n",
      "           1       0.00      0.00      0.00       318\n",
      "           2       0.00      0.00      0.00       881\n",
      "\n",
      "    accuracy                           0.60      3000\n",
      "   macro avg       0.20      0.33      0.25      3000\n",
      "weighted avg       0.36      0.60      0.45      3000\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-08T12:50:34.199Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Set your variables, separating your target from the rest of the features\n",
    "X = albert[['x1', 'x2', 'x3']]\n",
    "y = albert['target']\n",
    "#data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "#Split test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Model\n",
    "sig = svm.SVC(kernel='rbf', C=1, gama=0.001, decision_function_shape='ovo').fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "sig.fit(X_train, y_train)  \n",
    "\n",
    "# See your prediction score\n",
    "preds = sig.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "rmsle = np.sqrt(mean_squared_log_error( y_test, preds))\n",
    "F1_weight = f1_score(y_test, preds, average='weighted')\n",
    "F1_macro = f1_score(y_test, preds, average='macro')\n",
    "F1_micro = f1_score(y_test, preds, average='micro')\n",
    "print(\"\\n\\n\\n\\nSEE YOUR RMSE SCORE: %f\" % (rmse))\n",
    "print(\"SEE YOUR RMSEL SCORE: %f\" % (rmsle), \"\\n\")\n",
    "print(\"SEE YOUR F1_weight SCORE: %f\" % (F1_weight))\n",
    "print(\"SEE YOUR F1_macro SCORE: %f\" % (F1_macro))\n",
    "print(\"SEE YOUR F1_micro SCORE: %f\" % (F1_micro),\"\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oh, wait...our F1-Micro which computes our multiclass accuracy is low...what about class weight trick instead of StandardScaler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T13:38:29.024376Z",
     "start_time": "2021-06-08T13:38:24.412224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, class_weight='balanced', decision_function_shape='ovo', gamma=0.001)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "SEE YOUR RMSE SCORE: 1.282316\n",
      "SEE YOUR RMSEL SCORE: 0.717899 \n",
      "\n",
      "SEE YOUR F1_weight SCORE: 0.373947\n",
      "SEE YOUR F1_macro SCORE: 0.305431\n",
      "SEE YOUR F1_micro SCORE: 0.344667 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Set your variables, separating your target from the rest of the features\n",
    "X = albert[['x1', 'x2', 'x3']]\n",
    "y = albert['target']\n",
    "#data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "#Split test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Model\n",
    "sig = svm.SVC(kernel='rbf', C=1, gamma=0.001, class_weight= 'balanced', decision_function_shape='ovo').fit(X_train, y_train)\n",
    "\n",
    "# Fit Model\n",
    "sig.fit(X_train, y_train)  \n",
    "\n",
    "# See your prediction score\n",
    "preds = sig.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "rmsle = np.sqrt(mean_squared_log_error( y_test, preds))\n",
    "F1_weight = f1_score(y_test, preds, average='weighted')\n",
    "F1_macro = f1_score(y_test, preds, average='macro')\n",
    "F1_micro = f1_score(y_test, preds, average='micro')\n",
    "print(\"\\n\\n\\n\\nSEE YOUR RMSE SCORE: %f\" % (rmse))\n",
    "print(\"SEE YOUR RMSEL SCORE: %f\" % (rmsle), \"\\n\")\n",
    "print(\"SEE YOUR F1_weight SCORE: %f\" % (F1_weight))\n",
    "print(\"SEE YOUR F1_macro SCORE: %f\" % (F1_macro))\n",
    "print(\"SEE YOUR F1_micro SCORE: %f\" % (F1_micro),\"\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stepsize in the mesh, it alters the accuracy of the plotprint\n",
    "#to better understand it, just play with the value, change it and print it\n",
    "h = .01\n",
    "#create the mesh\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "# create the title that will be shown on the plot\n",
    "titles = ['Linear kernel','RBF kernel','Polynomial kernel','Sigmoid kernel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T16:48:49.994512Z",
     "start_time": "2021-06-07T16:19:15.590Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, clf in enumerate((linear, rbf, poly, sig)):\n",
    "    #defines how many plots: 2 rows, 2columns=> leading to 4 plots\n",
    "    plt.subplot(2, 2, i + 1) #i+1 is the index\n",
    "    #space between plots\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4) \n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.PuBuGn, alpha=0.7)\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.PuBuGn,     edgecolors='grey')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('target')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(titles[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T16:48:49.995509Z",
     "start_time": "2021-06-07T16:19:15.596Z"
    }
   },
   "outputs": [],
   "source": [
    "# Not many improvements after one hour of tuning..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T01:48:21.557728Z",
     "start_time": "2021-06-07T01:48:09.011Z"
    }
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T16:48:49.996506Z",
     "start_time": "2021-06-07T16:19:15.599Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# How many randoms?\n",
    "#randoms = pd.DataFrame(clf.predict_proba(X_test)[:, 1], columns=['prob']).query('0.2 < prob < 0.6 ')\n",
    "#randoms\n",
    "#print(randoms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T16:48:49.997503Z",
     "start_time": "2021-06-07T16:19:15.601Z"
    }
   },
   "outputs": [],
   "source": [
    "# 52 from 10000 is irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_pred = linear.predict(X_test)\n",
    "poly_pred = poly.predict(X_test)\n",
    "rbf_pred = rbf.predict(X_test)\n",
    "sig_pred = sig.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T16:48:49.998501Z",
     "start_time": "2021-06-07T16:19:15.605Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the predictions and put them on the test data.\n",
    "X_output = X_test.copy()\n",
    "X_output.loc[:,'predict'] = np.round(sig.predict(X_output),2)\n",
    "\n",
    "# Randomly pick some observations\n",
    "#random_picks = np.arange(1,330,50) # Every 50 rows\n",
    "predict_table = X_output.iloc[:]\n",
    "predict_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T16:48:49.998501Z",
     "start_time": "2021-06-07T16:19:15.607Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% Predict on your Unseen Data\n",
    "data = albert.sample(frac=0.95, random_state=786)\n",
    "data_unseen = albert.drop(data.index)\n",
    "\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data_unseen.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print('Data for Modeling: ' + str(data.shape))\n",
    "print('Unseen Data For Predictions ' + str(data_unseen.shape))\n",
    "data_unseen\n",
    "\n",
    "# Backup your old column to compare\n",
    "target_backup = [data_unseen[\"target\"]]\n",
    "target_backup = pd.DataFrame(target_backup).T\n",
    "del data_unseen['target']\n",
    "\n",
    "# Get the predictions and put them with the unseen data.\n",
    "X_output = data_unseen.copy()\n",
    "X_output.loc[:,'predict'] = np.round(sig.predict(X_output),2)\n",
    "\n",
    "# Randomly pick some observations\n",
    "#random_picks = np.arange(1,330,50) # Every 50 rows\n",
    "predict_table = X_output.iloc[:]\n",
    "\n",
    "# Fill your new table with all variables to compare\n",
    "predict_table['target'] = target_backup\n",
    "predict_table = predict_table[['x1', 'x2', 'x3', 'target', 'predict']]\n",
    "# Boolean function for new column\n",
    "def Check(predict_table):\n",
    "   if predict_table['target']== predict_table['predict']:\n",
    "      return \"True\"\n",
    "   else:\n",
    "      return \"False\" \n",
    "# Result\n",
    "predict_table['result'] = predict_table.apply(Check, axis=1)\n",
    "predict_table\n",
    "\n",
    "predict_table['result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T16:48:49.999498Z",
     "start_time": "2021-06-07T16:19:15.609Z"
    }
   },
   "outputs": [],
   "source": [
    "# It's good to compare but with parcimony: since it's a hierarchy from low/medium/high, True or False doesn't always has the same weight;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
